# Config file for running an experiment with DeepRL agents.


output_dir: 'test_experiment'

# Ray configs
ray_config:
  num_cpus: 20
  cpus_per_trial: 1
  num_gpus: 0
  gpus_per_trial: 0

# Each element in trial_sets contains:
#   + number of replications to be run
#   + list of seeds to be used
#   + a name for the trial_set
#   + the configs needed to specify the trials
trial_sets:

  - trial_set_name: 'Deep_Q_trials'
    num_replications: 1
    seeds: []

    trial:
      # Environment configs
      envs_module_name: 'main.core.envs'
      env_config:
        class: 'RandomMDPEnv'
        args: [5, 5, 'r5', 'c4']  # [num_states, num_actions, reward_fnc, cost_fnc]
        kwargs:
          transition_seed: 1492
      
      # Agent configs
      agents_module_name: 'main.core.agents'
      agent_config:
        class: 'DeepRVIQLearningBasedAgent'
        args: [100_000, 256, 0.01, 0.001] # [buffer_maxlen, batchsize, q_lr, rho_lr]
        kwargs: {eps: 0.0001, enable_cuda: False}
      
      # IOManager configs
      iomanager_config:
        class: Null  # Will break if not specified at runtime
        args: ['test_data']  # Output directory
        kwargs:
          print_interval: 1000
          log_interval: 1000
          agent_name: Null  # specified at runtime
          filename: 'ratios'  # specified at runtime
      
      # Trial configs
      trial_config:
        width: 1000  # Width of window for computing moving averages
        n_steps: 200_000
        n_episodes: 1
        log: True
        plot: False
        print: True
